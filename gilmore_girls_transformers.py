# -*- coding: utf-8 -*-
"""Gilmore Girls Transformers.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/126SGHKypHTmeNkT-WlQ0gMSqDd5PbAHL
"""

!pip install transformers

from transformers import AutoTokenizer

# Load the tokenizer
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

# Your text
text = "Hello, this is a test text for the tokenizer."

# Tokenize the text
tokens = tokenizer(text)

print(tokens)

import pandas as pd
import io
from google.colab import files


uploaded = files.upload()

#Season 1
from google.colab import files
import pandas as pd

season1_df = pd.read_csv('/content/season1.csv', skiprows=3, engine='python', header = None)
season1_df = season1_df[:-4]
season1_df = season1_df.applymap(lambda x: x.replace('"', '') if isinstance(x, str) else x)
season1_df.head()

#Season 2
season2_df = pd.read_csv('/content/season2.csv', skiprows=3, engine='python', header = None)
season2_df = season2_df[:-1]
season2_df = season2_df.applymap(lambda x: x.replace('"', '') if isinstance(x, str) else x)
season2_df.head()

#Season 3
season3_df = pd.read_csv('/content/season3.csv', skiprows=4, engine='python', header = None)
season3_df = season3_df.applymap(lambda x: x.replace('"', '') if isinstance(x, str) else x)
season3_df.head()

#Season 4
season4_df = pd.read_csv('/content/season4.csv', engine='python', header = None)
last_four_season4 = season4_df.tail(3)
season4_df = season4_df[:-11]
season4_df = season4_df.applymap(lambda x: x.replace('"', '') if isinstance(x, str) else x)
season4_df.head()

#Season 5
season5_df = pd.read_csv('/content/season5.csv', engine='python', header = None)
season5_df = pd.concat([last_four_season4, season5_df], ignore_index=True)
season5_df = season5_df[:-2]
season5_df = season5_df.applymap(lambda x: x.replace('"', '') if isinstance(x, str) else x)
season5_df = season5_df.applymap(lambda x: x.replace('[OS]', '') if isinstance(x, str) else x)
season5_df.head()

#Season 6
season6_df = pd.read_csv('/content/season6.csv', skiprows=1, engine='python', header = None)
season6_df = season6_df.applymap(lambda x: x.replace('"', '') if isinstance(x, str) else x)
season6_df.head()

#Season 7
season7_df = pd.read_csv('/content/season7.csv', skiprows=1, engine='python', header = None)
season7_df = season7_df.applymap(lambda x: x.replace('"', '') if isinstance(x, str) else x)
season5_df = season5_df[:-3]
season7_df.head()

#concatenate to one csv file
season1_df['Season'] = 1
season2_df['Season'] = 2
season3_df['Season'] = 3
season4_df['Season'] = 4
season5_df['Season'] = 5
season6_df['Season'] = 6
season7_df['Season'] = 7


all_gg_df = pd.concat([season1_df, season2_df, season3_df, season4_df, season5_df, season6_df, season7_df], ignore_index=True)

all_gg_df.head()

all_gg_df.to_csv('all_gg_df.csv', index=False)

# Download the combined file to your local system
files.download('all_gg_df.csv')

all_gg_df.head()

#turn it into a text file
all_txt = "\n".join(all_gg_df[0].tolist())
all_txt[0:100]

#tokenize
tokenized = tokenizer(all_txt)

len(tokenized['input_ids'])

with open('ggtokenized.csv', 'w', newline='') as file:
    tokenized_str = str(tokenized)
    file.write(tokenized_str)

import numpy as np

tokens = np.array(tokenized['input_ids'])

np.save("tokenized.npy", tokens)

tokens = np.load("tokenized.npy")

bos_token = '<BOS>'
eos_token = '<EOS>'

# Add new special tokens to the tokenizer
special_tokens_dict = {'bos_token': bos_token, 'eos_token': eos_token}
num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)

import glob
import numpy as np
import torch
from torch.utils.data import Dataset, DataLoader

class GilmoreGirlsDataset(Dataset):
  def __init__(self, data, block_size):
    # just loading the data
    self.data = data.tolist()
    self.block_size = block_size

  def __getitem__(self, idx):
    # return target by idx
    start_idx = idx
    end_idx = start_idx + self.block_size + 1
    chunk = self.data[start_idx:end_idx]
    if chunk[0] != 30522:
      chunk = [30522] + chunk
    if chunk[-1] != 30523:
      chunk = chunk + [30523]
    chunk = torch.tensor(chunk)
    input_ids = chunk[:-1]
    target_ids = chunk[1:]
    return input_ids, target_ids

  def __len__(self):
    # len(dataset)
    return len(self.data) - self.block_size - 1

# takes care of batching

bos_token_id = tokenizer.bos_token_id
eos_token_id = tokenizer.eos_token_id

print("BOS token ID:", bos_token_id)
print("EOS token ID:", eos_token_id)

import torch.nn.functional as F
import torch
from torch import nn

class Attention(nn.Module):
  def __init__(self, embedding_dim):
    super().__init__()
    self.query = nn.Linear(embedding_dim, embedding_dim)
    self.keys = nn.Linear(embedding_dim, embedding_dim)
    self.values = nn.Linear(embedding_dim, embedding_dim)

  def forward(self, idx):
    # shape of idx ? [B, T, C], C = embedding dim
    B,T,C = idx.shape
    q = self.query(idx) # [B, T, C] -> [B, T, C]
    k = self.keys(idx) # [B, T, C] -> [B, T, C]
    # q @ k -> ignore batch dimension, only look at [T, C] x [T, C] <-- this wont work, since inner dims have to mathc
    # so... we transpose k from [B, T, C] -> [B, C, T]
    wei = q @ k.transpose(-2,-1) * C**-0.5 # [B, T, C] x [B, C, T] --> [B, T, T]

    tril = torch.tril(torch.ones(T, T)).to("cuda")
    wei = wei.masked_fill(tril == 0, float('-inf'))
    # wei shape: [B, T, T]
    wei = F.softmax(wei, dim=-1)
    v = self.values(idx) # [B, T, C]
    # wei shape: [B, T, T]

    #[B, T, T] x [B, T, C] --> [B, T, C]

    out = wei @ v
    return out

class FFN(nn.Module):
  def __init__(self, embedding_dim):
    super().__init__()
    self.big = nn.Linear(embedding_dim, 4*embedding_dim)
    self.act = nn.ReLU()
    self.back_down = nn.Linear(4*embedding_dim, embedding_dim)

  def forward(self, idx):
    # idx: shape = [B, T, C]
    out_proj = self.big(idx) # shape: [B, T, 4*C]
    act = self.act(out_proj) # shape: [B, T, 4*C]
    final = self.back_down(act) # shape: [B, T, C]
    return final

class AttentionBlock(nn.Module):
  def __init__(self, embedding_dim):
    super().__init__()
    self.attn = Attention(embedding_dim)
    self.ffn = FFN(embedding_dim)

  def forward(self, idx):
    # idx shape: [B, T, C]
    attended = self.attn(idx) # shape: [B, T, C]
    ffn_d = self.ffn(attended) # shape: [B, T, C]
    return ffn_d

class GilmoreGirlsModel(nn.Module):
    def __init__(self, vocab_size, embedding_dim, max_seqlen=384, num_blocks=4):
        super().__init__()
        self.embeddings_table = nn.Embedding(vocab_size, embedding_dim)
        self.pos_emb = nn.Embedding(max_seqlen, embedding_dim)
        self.proj = nn.Linear(embedding_dim, vocab_size)
        self.blocks = nn.ModuleList([AttentionBlock(embedding_dim) for _ in range(num_blocks)])
        self.max_seqlen = max_seqlen

    def forward(self, index):
        #index shape: [B, T]
        x = self.embeddings_table(index) # B, T, C
        B, T, C = x.shape
        pe = self.pos_emb(torch.arange(T, device=index.device).repeat(B, 1))

        x = x + pe

        for block in self.blocks:
            x = x + block(x)

        logits = self.proj(x)
        return logits

    def generate(self, idx, max_new_tokens, top_k=5):
      # idx is (B, T) array of indices in the current context
      for _ in range(max_new_tokens):
          # Get the predictions
          logits = self(idx)
          # Focus only on the last time step
          logits = logits[:, -1, :]  # becomes (B, C)

          # Filter to top-k tokens
          top_k_logits, top_k_indices = torch.topk(logits, k=top_k, dim=-1)
          probs = F.softmax(top_k_logits, dim=-1)

          # Sample from the top-k distribution
          idx_next = torch.multinomial(probs, num_samples=1)
          idx_next = top_k_indices.gather(-1, idx_next)

          # Append sampled index to the running sequence
          idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)

      return idx

# Commented out IPython magic to ensure Python compatibility.
# %env CUDA_LAUNCH_BLOCKING=1

def get_n_params(model):
    pp=0
    for p in list(model.parameters()):
        nn=1
        for s in list(p.size()):
            nn = nn*s
        pp += nn
    return pp

#training loop

batch_size = 16
vocab_size = len(tokenizer)
m = GilmoreGirlsModel(vocab_size=vocab_size, embedding_dim=256, max_seqlen=384, num_blocks=8)
print(get_n_params(m))
m = m.to("cuda")
optimizer = torch.optim.AdamW(m.parameters(), lr = 1e-3)

dl = GilmoreGirlsDataset(tokens, block_size=382)
dataloader = DataLoader(dl, batch_size=batch_size, shuffle=True)
import time
for epoch in range(20000):
  for steps, (xb, yb) in enumerate(dataloader):
    t = time.time()
    logits = m(xb.to("cuda")) #B, T, C
    B, T, C = logits.shape
    logits = logits.view(B*T, C) #B*T, C

    yb = yb.view(B*T) #B*T
    loss = F.cross_entropy(logits, yb.to("cuda")) #1

    loss.backward()
    optimizer.step()
    # print(f'time: {time.time()-t}')
    if steps%200 == 0:
      print('step', steps, 'loss', loss.item()) # B, T
      ids = m.generate(idx = torch.tensor(tokenizer.bos_token_id, dtype=torch.long).unsqueeze(0).unsqueeze(0).to("cuda"), max_new_tokens=100)
      decoded_text = tokenizer.decode(ids[0], skip_special_tokens=True)
      print(decoded_text)
    optimizer.zero_grad()


print(loss.item())

